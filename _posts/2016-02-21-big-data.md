---
layout: blog
title:  "Big Data Specialization"
date:   2016-02-21 12:00:00
categories: data
permalink: /big-data
author: Kylin Soong
duoshuoid: ksoong2016022101
excerpt: Introduction to Big Data, Hadoop Platform and Application Framework, Introduction to Big Data Analytics, Machine Learning With Big Data, Graph Analytics for Big Data, Big Data - Capstone Project
---

* Table of contents
{:toc}

## Introduction to Big Data

[Natasha Balac](https://www.linkedin.com/in/natasha-balac-1992964). SDSC - [San Diego Supercomputer Center](http://www.sdsc.edu/).

**Outpacing Time** - _Predicting the Future to Solve Tomorrow's Business Challenges_.

![Data Source to Insight]({{ site.baseurl }}/assets/blog/bigddata-data-to-insight.png)

Big Data is making the world **go around**.

![world go around]({{ site.baseurl }}/assets/blog/using-it-is-hardest-part.jpg)

**Data Growth** - Driven by Unstructured Data.

![Unstructured Data]({{ site.baseurl }}/assets/blog/bigdata-tendencias-storage.jpg)

1. **Zettabyte** - 1 « 70 -> 1 * (2^70)
2. **Exabyte** - 1 « 60 -> 1 * (2^60)
3. **Petabyte** - 1 « 50 -> 1 * (2^50)
4. **Terabyte** - 1 « 40 -> 1 * (2^40)
5. **Gigabyte** - 1 « 30 -> 1 * (2^30) 

### Getting Started in Hadoop

**Hardware and Software Requirements**

* Fedora 20, VirtualBox 5+
* intel CORE i5, 8GB RAM, 20GB Disk
* [Cloudera QuickStart VM](http://www.cloudera.com/content/www/en-us/downloads/quickstart_vms/5-4.html?src=Coursera)

**Install Cloudera QuickStart VM with VirtualBox**

1. Go to [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads) to download and install VirtualBox for your computer. A alternative way, download [virtualbox.repo]({{ site.baseurl }}/assets/download/virtualbox.repo), yum install via execute `yum install VirtualBox-5.0`.
2. Download the VirtualBox VM from [https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.4.2-0-virtualbox.zip](https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.4.2-0-virtualbox.zip).
3. Unzip the VirtualBox VM
4. Start VirtualBox
5. Import the VM to VirtualBox

**Additional Resource**

1. [Apache Hadoop web site](http://hadoop.apache.org/)
2. [Apache Hadoop shell command guide](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html)
3. [More about Cloudera QuickStart Virtual Machine](http://www.cloudera.com/documentation/enterprise/latest/topics/cloudera_quickstart_vm.html)

**Why Hadoop?** - Low-cost, Scalable, Fault tolerant, Flexible

## Hadoop Platform and Application Framework

![Hadoop logo]({{ site.baseurl }}/assets/blog/hadoop-logo.jpg)

Apache Hadoop is an open source software framework for storage and large scale processing of data-sets on clusters of commodity hardware.

[**Doug Cutting**](https://www.linkedin.com/in/cutting), [**Mike Cafarella**](https://en.wikipedia.org/wiki/Mike_Cafarella)

### Big Data Hadoop Stack

**Highlights of Hadoop**

* Moving Computation to Data

![Hadoop highlight 1]({{ site.baseurl }}/assets/blog/hadoop-highlisht-movecomputationtodata.png)

* Scalability at Hadoop’s core

![Hadoop highlight 2]({{ site.baseurl }}/assets/blog/haddop-highlisht-scalability.png)

* Reliability
* New Approach to Data - unstructured/semi-structured
* New Kinds of Analysis

**Hadoop Modules**

![Hadoop Modules]({{ site.baseurl }}/assets/blog/hadoop-modules.png)

* _Hadoop Common_ - contains libraries and utilities needed by other Hadoop modules.
* _Hadoop Distributed File System(HDFS)_ - is a distributed file system that stores data on a commodity machine. Providing very high aggregate bandwidth across the entire cluster.
* _Hadoop YARN_ - is a resource management platform responsible for managing compute resources in the cluster and using them in order to schedule users and applications.
* _Hadoop MapReduce_ is a programming model that scales data across a lot of different processes. 

![Hadoop-Echosystem.jpg]({{ site.baseurl }}/assets/blog/Hadoop-Echosystem.jpg)

* Sqoop - [Apache Sqoop](#Apache Sqoop)
* HBase - Column-oriented database management system, Key-value store, Based on Google Big Table, Can hold extremely large data, Dynamic data model, Not a Relational DBMS. [hbase.apache.org](https://hbase.apache.org/).
* Pig - PIG Highlevel programming on top of Hadoop MapReduce, The language: Pig Latin, Data analysis problems as data flows, Originally developed at Yahoo 2006.
* Hive - Data warehouse software facilitates querying and managing large datasets residing in distributed storage.
* Oozie - A workflow scheduler system to manage Apache Hadoop jobs.
* Zookeeper - Provides operational services for a Hadoop cluster group services. Centralized service for: maintaining configuration information naming services, **providing distributed synchronization and providing group services**
* Flume - [Apache Flume](#Apache Flume)

#### Apache Sqoop

[sqoop.apache.org/](http://sqoop.apache.org/)

![Apache Sqoop]({{ site.baseurl }}/assets/blog/apache-sqoop.jpg)

Apache Sqoop is a tool that uses MapReduce to transfer data between Hadoop clusters and relational databases very efficiently. It works by spawning tasks on multiple data nodes to download various portions of the data in parallel. When you're finished, each piece of data is replicated to ensure reliability, and spread out across the cluster to ensure you can process it in parallel on your cluster.

An Example:

~~~
$ sqoop import-all-tables \
    -m 1 \
    --connect jdbc:mysql://quickstart:3306/test_db \
    --username=test_user \
    --password=test_pass \
    --compression-codec=snappy \
    --as-avrodatafile \
    --warehouse-dir=/user/hive/warehouse
~~~

#### Apache Flume

[flume.apache.org](https://flume.apache.org/)

![Apache Flume]({{ site.baseurl }}/assets/blog/apache-flume.png)

Flume is a scalable real-time framework that allows you to route, filter and aggregate in to all kinds of mini-operations on the data as you transfer it on its way to scalable processing platform like a Hadoop.

### Overview of the Hadoop Stack

 
