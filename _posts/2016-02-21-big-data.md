---
layout: blog
title:  "Big Data Specialization"
date:   2016-02-21 12:00:00
categories: data
permalink: /big-data
author: Kylin Soong
duoshuoid: ksoong2016022101
excerpt: Introduction to Big Data, Hadoop Platform and Application Framework, Introduction to Big Data Analytics, Machine Learning With Big Data, Graph Analytics for Big Data, Big Data - Capstone Project
---

* Table of contents
{:toc}

## Introduction to Big Data

SDSC - [San Diego Supercomputer Center](http://www.sdsc.edu/).

* The area of data science will be the number one catalyst for economic growth.
* Mobile Catalyzing Big Data - 移动客户端使数据增加的速度相较之前，增加了一个数量级(More than one billion people login Facebook every day.)
* Cloud Computing Catalyzing Big Data - Computing anywhere and anytime, On-Demand Computing
* To summarize, a new torrent of Big Data combined with computing  capability anytime, anywhere has been at the core of the launch of the big data era.
* Big Data will enable better models which allows for higher precision recommendations or solutions to make the world a better place

**Outpacing Time** - _Predicting the Future to Solve Tomorrow's Business Challenges_.

![Data Source to Insight]({{ site.baseurl }}/assets/blog/bigddata-data-to-insight.png)

Big Data is making the world **go around**.

![world go around]({{ site.baseurl }}/assets/blog/using-it-is-hardest-part.jpg)

**Data Growth** - Driven by Unstructured Data.

![Unstructured Data]({{ site.baseurl }}/assets/blog/bigdata-tendencias-storage.jpg)

1. **Zettabyte** - 1 « 70 -> 1 * (2^70)
2. **Exabyte** - 1 « 60 -> 1 * (2^60)
3. **Petabyte** - 1 « 50 -> 1 * (2^50)
4. **Terabyte** - 1 « 40 -> 1 * (2^40)
5. **Gigabyte** - 1 « 30 -> 1 * (2^30) 

### Where does Big Data come from?

* Machine-Generated Data - Contribute most of data to Big Data, Real-time action, **IOT**( Internet of Things)
* Big Data Generated by People - The Unstructured Data Challenge, How it being used
** Hadoop - Hadoop us designed to support the processing of large data sets in a distribute computing environment.
** Data Warehouse - Extract, Transform, Load.

**Layered processing wat to find Value from Big Data**

~~~
Multiple Data Sources
~~~

~~~
Retrieval and Storage
~~~

~~~
Pre-Processing
~~~

~~~
Analysis
~~~

~~~
Value
~~~

* Organization-Generated Data: Structured But Often Siloed, Benefits come from combining with other data types.

**The Key: Integrating Diverse Data**

### Characteristics of Big Data and Dimensions of Scalability

* 3 main dimensions: Volume, Velocity, Variety.
* 6V of Big Data: Volume, Velocity, Variety, Veracity, Valence, Value

* Volume - Volume refers to the vast amounts of data that is generated every second, minutes, hour, and day in our digitzed world.
** Volume == Size

* Velocity - Velocity refers to the speed at which data is being generated and the pace at which data moves from one point to the next.
** Velocity = Speed(speed of creating data, speed of storing data, speed of analyzing data)
** V = X / V

* Variety - Variety refers to the every increasing different forms of data that can come in 
** Variety == Complexity. The heterogeneity of data can be characterized along several dimensions.

* Veracity - Veracity refers to the biases, noise, and abnormality in data, or better yet, it refers to the often unmeasurable uncertainties and truthfulness and trustworthiness of data.
** Veracity == Quality

* Valence - Valence refers to the connectedness of Big Data in the form of graphs.
** Valence == Connectedness

* Value

**A "Small" Definition of Big Data**

* [http://words.sdsc.edu/words-data-science/big-data](http://words.sdsc.edu/words-data-science/big-data)
* [https://www.coursera.org/learn/intro-to-big-data/supplement/CHItC/a-small-definition-of-big-data](https://www.coursera.org/learn/intro-to-big-data/supplement/CHItC/a-small-definition-of-big-data)

### Getting Value out of Big Data

[Five P of modern data science](http://words.sdsc.edu/words-data-science/data-science):

1. People
2. Purpose
3. Process(ACQUIRE, PREPARE, ANALYZE, REPORT, ACT)
4. Platforms(Hadoop)
5. Programmability

Steps in the Data Science Process:

1. Acquiring Data
2. Exploring Data and Pre-processing Data
3. Analyzing Data
4. Communicating Results
5. Turning insignts into Action

### Foundations for Big Data Systems and Programming

Distributed File System(DFS) Provides:

* Data Scalability
* Fault Tolerance
* High Concurrency

The Key of DFS:

* Partitioning
* Replication

Commodity Cluster: Affordable, Less-specialized

* Redundant data storage
* Data-parallel job restart

Programming Models for Big Data - MapReduce

### Getting Started in Hadoop

This section contain contents of getting start with Hadoop/Big Data.

#### Layered Hadoop ecosystem framwork

**HDFS**  

the fundation of Hadoop ecosystem framwork/Big Data, a storage layer, 

two capabilities that are essential for managing big data:

* Scalability to large data sets - Partitioning or Splitting large files across multiple computers. This allows parallel access to very large files since the computations run in parallel on each node where the data is stored. 
* Reliability to cope with hardware failures. - Replication

64 MB - The default chunk size, the size of each piece of a file is 64 megabytes. But you can configure this to any size. 

Two key components:

1. NameNode
2, DataNode

**YARN**

The Resource Manager for Hadoop. 

**MapReduce** - Simple Programming for big results.

MapReduce is a programming model for the Hadoop ecosystem. It relies on YARN to schedule and execute parallel processing over the distributed file blocks in HDFS. 

MapReduce hides complexities of parallel programming and greatly simplifies building parallel applications. 

[MapReduce in the Pasta Sauce example](https://www.coursera.org/learn/intro-to-big-data/supplement/zRl3m/mapreduce-in-the-pasta-sauce-example)

**Cloud Computing: An Important Big Data Enabler**

The main idea behind cloud computing is to transform computing infrastructure into a commodity, this is what Big Data try to achieve in data processing/persisting.



**Hardware and Software Requirements**

* Fedora 20, VirtualBox 5+
* intel CORE i5, 8GB RAM, 20GB Disk
* [Cloudera QuickStart VM](http://www.cloudera.com/content/www/en-us/downloads/quickstart_vms/5-4.html?src=Coursera)

**Install Cloudera QuickStart VM with VirtualBox**

1. Go to [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads) to download and install VirtualBox for your computer. A alternative way, download [virtualbox.repo]({{ site.baseurl }}/assets/download/virtualbox.repo), yum install via execute `yum install VirtualBox-5.0`.
2. Download the VirtualBox VM from [https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.4.2-0-virtualbox.zip](https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.4.2-0-virtualbox.zip).
3. Unzip the VirtualBox VM
4. Start VirtualBox
5. Import the VM to VirtualBox

**Additional Resource**

1. [Apache Hadoop web site](http://hadoop.apache.org/)
2. [Apache Hadoop shell command guide](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html)
3. [More about Cloudera QuickStart Virtual Machine](http://www.cloudera.com/documentation/enterprise/latest/topics/cloudera_quickstart_vm.html)

**Why Hadoop?** - Low-cost, Scalable, Fault tolerant, Flexible

## Hadoop Platform and Application Framework

![Hadoop logo]({{ site.baseurl }}/assets/blog/hadoop-logo.jpg)

Apache Hadoop is an open source software framework for storage and large scale processing of data-sets on clusters of commodity hardware.

[**Doug Cutting**](https://www.linkedin.com/in/cutting), [**Mike Cafarella**](https://en.wikipedia.org/wiki/Mike_Cafarella)

### Big Data Hadoop Stack

**Highlights of Hadoop**

* Moving Computation to Data

![Hadoop highlight 1]({{ site.baseurl }}/assets/blog/hadoop-highlisht-movecomputationtodata.png)

* Scalability at Hadoop’s core

![Hadoop highlight 2]({{ site.baseurl }}/assets/blog/haddop-highlisht-scalability.png)

* Reliability
* New Approach to Data - unstructured/semi-structured
* New Kinds of Analysis

**Hadoop Modules**

![Hadoop Modules]({{ site.baseurl }}/assets/blog/hadoop-modules.png)

* _Hadoop Common_ - contains libraries and utilities needed by other Hadoop modules.
* _Hadoop Distributed File System(HDFS)_ - is a distributed file system that stores data on a commodity machine. Providing very high aggregate bandwidth across the entire cluster.
* _Hadoop YARN_ - is a resource management platform responsible for managing compute resources in the cluster and using them in order to schedule users and applications.
* _Hadoop MapReduce_ is a programming model that scales data across a lot of different processes. 

![Hadoop-Echosystem.jpg]({{ site.baseurl }}/assets/blog/Hadoop-Echosystem.jpg)

* Sqoop - [Apache Sqoop](#Apache Sqoop)
* HBase - Column-oriented database management system, Key-value store, Based on Google Big Table, Can hold extremely large data, Dynamic data model, Not a Relational DBMS. [hbase.apache.org](https://hbase.apache.org/).
* Pig - PIG Highlevel programming on top of Hadoop MapReduce, The language: Pig Latin, Data analysis problems as data flows, Originally developed at Yahoo 2006.
* Hive - Data warehouse software facilitates querying and managing large datasets residing in distributed storage.
* Oozie - A workflow scheduler system to manage Apache Hadoop jobs.
* Zookeeper - Provides operational services for a Hadoop cluster group services. Centralized service for: maintaining configuration information naming services, **providing distributed synchronization and providing group services**
* Flume - [Apache Flume](#Apache Flume)

#### Apache Sqoop

[sqoop.apache.org/](http://sqoop.apache.org/)

![Apache Sqoop]({{ site.baseurl }}/assets/blog/apache-sqoop.jpg)

Apache Sqoop is a tool that uses MapReduce to transfer data between Hadoop clusters and relational databases very efficiently. It works by spawning tasks on multiple data nodes to download various portions of the data in parallel. When you're finished, each piece of data is replicated to ensure reliability, and spread out across the cluster to ensure you can process it in parallel on your cluster.

An Example:

~~~
$ sqoop import-all-tables \
    -m 1 \
    --connect jdbc:mysql://quickstart:3306/test_db \
    --username=test_user \
    --password=test_pass \
    --compression-codec=snappy \
    --as-avrodatafile \
    --warehouse-dir=/user/hive/warehouse
~~~

#### Apache Flume

[flume.apache.org](https://flume.apache.org/)

![Apache Flume]({{ site.baseurl }}/assets/blog/apache-flume.png)

Flume is a scalable real-time framework that allows you to route, filter and aggregate in to all kinds of mini-operations on the data as you transfer it on its way to scalable processing platform like a Hadoop.

### Overview of the Hadoop Stack

 
